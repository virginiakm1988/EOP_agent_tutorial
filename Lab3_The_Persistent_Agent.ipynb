{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lab 3: The Persistent Agent — Memory, State, and Multi-Turn Coherence\n",
        "\n",
        "**Series**: Agentic Engineering Crash Course  \n",
        "**Module**: 3 — Memory & State (Managing Context Persistence)  \n",
        "**Prerequisites**: Lab 1 and Lab 2 (or familiarity with tool selection and Pydantic tools), Python 3.10+, OpenAI API key  \n",
        "\n",
        "---\n",
        "\n",
        "## How to use this tutorial in Google Colab\n",
        "\n",
        "1. Open [Google Colab](https://colab.research.google.com/) and create a new notebook.\n",
        "2. For each **markdown section** below: insert a **Text cell** and paste the section.\n",
        "3. For each **code block**: insert a **Code cell** and paste the code, then run.\n",
        "4. Run cells in order from top to bottom.\n",
        "\n",
        "**Suggested time**: 45–60 min.  \n",
        "**Experiments**: Baseline (required). Exploration: Experiments 1–3 required; Experiment 4 optional.\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Learning Objectives\n",
        "\n",
        "By the end of this lab you will be able to:\n",
        "\n",
        "1. **Implement** conversation-history memory by appending messages to the prompt.\n",
        "2. **Observe** context-window overflow and its effect on agent behavior.\n",
        "3. **Build** a summarization strategy to compress history while preserving task-relevant state.\n",
        "4. **Use** an external state object (dict/dataclass) to persist structured data across turns.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Theoretical Why: Why Memory Matters\n",
        "\n",
        "### Mechanism\n",
        "\n",
        "LLMs are **stateless functions**: \\( f(\\text{prompt}) \\to \\text{completion} \\). All \"memory\" is an illusion created by including prior messages in the prompt. There is no persistent state inside the model; every turn is conditioned only on what you send.\n",
        "\n",
        "Concepts to keep in mind:\n",
        "\n",
        "- **Context window as a finite resource**: Only a fixed number of tokens are available. Every message (system, user, assistant, tool) competes for this budget. Once you exceed it, the API may truncate, reject, or behave unpredictably.\n",
        "- **Recency bias**: The model attends more strongly to recent tokens. Information from many turns ago may be effectively \"forgotten\" even if it is still present in the prompt.\n",
        "- **Explicit state vs. implicit state**: You can maintain **structured state** (e.g. a scratchpad, JSON blob, or dataclass in the system message) that the model reads and updates, or rely on **implicit state** (raw conversation history). Explicit state is more reliable for exact values (e.g. ticket IDs, counts) that must not be lost.\n",
        "\n",
        "**Maintenance connection**: Multi-turn EOP agent failures often trace to **lost context** — the agent \"forgets\" a prior tool result or user instruction because it was pushed out of the effective attention window or truncated. The first place to look is token count, history length, and whether critical facts are in a structured state object.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Setup\n",
        "\n",
        "**Dependencies**: Python 3.10+, `openai`, `tiktoken` (for token counting).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell: Install dependencies\n",
        "!pip install -q openai tiktoken\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c321c5f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell: Imports and API key (OpenAI or NVIDIA NIM)\n",
        "import json\n",
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "import tiktoken\n",
        "from openai import OpenAI\n",
        "\n",
        "use_nim = os.environ.get(\"USE_NIM\", \"\").lower() in (\"1\", \"true\", \"yes\") or \"NIM_API_KEY\" in os.environ\n",
        "if use_nim:\n",
        "    if \"NIM_API_KEY\" not in os.environ:\n",
        "        os.environ[\"NIM_API_KEY\"] = getpass(\"Enter your NVIDIA API key (NIM): \")\n",
        "    client = OpenAI(\n",
        "        base_url=\"https://integrate.api.nvidia.com/v1\",\n",
        "        api_key=os.environ[\"NIM_API_KEY\"],\n",
        "    )\n",
        "    MODEL = os.environ.get(\"NIM_MODEL\", \"nvidia/llama-3.3-nemotron-super-49b-v1.5\")\n",
        "else:\n",
        "    if \"OPENAI_API_KEY\" not in os.environ:\n",
        "        os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter your OpenAI API key: \")\n",
        "    client = OpenAI()\n",
        "    MODEL = \"gpt-4o-mini\"\n",
        "ENCODING = tiktoken.encoding_for_model(\"gpt-4\")  # Approximate token count; works for both backends\n",
        "print(f\"Using model: {MODEL}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 4. Baseline Code: Multi-Turn Loop with Conversation History\n",
        "\n",
        "We build a simple multi-turn loop: the user sends messages, the agent responds (using a single tool for simplicity). We maintain a `messages` list that grows each turn and count tokens so we can observe context growth.\n",
        "\n",
        "### 4.1 Token counting and message list\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell: Token count and message list\n",
        "\n",
        "def count_tokens(messages: list[dict]) -> int:\n",
        "    \"\"\"Approximate token count for the messages list (OpenAI chat format).\"\"\"\n",
        "    total = 0\n",
        "    for m in messages:\n",
        "        total += 4  # overhead per message\n",
        "        total += len(ENCODING.encode(str(m.get(\"content\", \"\"))))\n",
        "        if m.get(\"tool_calls\"):\n",
        "            for tc in m[\"tool_calls\"]:\n",
        "                total += len(ENCODING.encode(json.dumps(tc)))\n",
        "    return total\n",
        "\n",
        "\n",
        "# We'll use a minimal tool: one function the model can call\n",
        "TOOLS_OPENAI = [\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"get_fact\",\n",
        "            \"description\": \"Store or retrieve a fact. Use store_fact to save a key-value fact; use get_fact to retrieve by key.\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\"key\": {\"type\": \"string\"}, \"value\": {\"type\": \"string\"}},\n",
        "                \"required\": [\"key\"],\n",
        "            },\n",
        "        },\n",
        "    }\n",
        "]\n",
        "\n",
        "def run_tool(name: str, arguments: dict, state: dict) -> str:\n",
        "    \"\"\"Simulated tool: in-memory key-value store.\"\"\"\n",
        "    if name != \"get_fact\":\n",
        "        return \"Unknown tool\"\n",
        "    key = arguments.get(\"key\", \"\")\n",
        "    value = arguments.get(\"value\")\n",
        "    if value is not None:\n",
        "        state[\"facts\"] = state.get(\"facts\", {})\n",
        "        state[\"facts\"][key] = value\n",
        "        return f\"Stored: {key} = {value}\"\n",
        "    return str(state.get(\"facts\", {}).get(key, \"(not set)\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.2 Multi-turn agent loop\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell: Multi-turn loop\n",
        "\n",
        "SYSTEM_PROMPT = (\n",
        "    \"You are a helpful assistant with access to a fact store. \"\n",
        "    \"When the user tells you something to remember, use the get_fact tool with both 'key' and 'value'. \"\n",
        "    \"When the user asks what you know, use get_fact with just 'key' to retrieve. \"\n",
        "    \"Keep responses brief.\"\n",
        ")\n",
        "\n",
        "def run_turn(messages: list[dict], state: dict):\n",
        "    \"\"\"Run one assistant turn: call API, handle tool calls, return updated messages and state.\"\"\"\n",
        "    response = client.chat.completions.create(\n",
        "        model=MODEL,\n",
        "        temperature=0.0,\n",
        "        messages=messages,\n",
        "        tools=TOOLS_OPENAI,\n",
        "        tool_choice=\"auto\",\n",
        "        max_tokens=200,\n",
        "    )\n",
        "    msg = response.choices[0].message\n",
        "    messages.append({\"role\": \"assistant\", \"content\": msg.content or \"\", \"tool_calls\": getattr(msg, \"tool_calls\", None) or []})\n",
        "\n",
        "    # Execute tool calls and append results\n",
        "    for tc in (msg.tool_calls or []):\n",
        "        name = tc.function.name\n",
        "        args = json.loads(tc.function.arguments)\n",
        "        result = run_tool(name, args, state)\n",
        "        messages.append({\n",
        "            \"role\": \"tool\",\n",
        "            \"tool_call_id\": tc.id,\n",
        "            \"content\": result,\n",
        "        })\n",
        "    return messages, state\n",
        "\n",
        "\n",
        "# Run a short multi-turn dialogue\n",
        "messages = [{\"role\": \"system\", \"content\": SYSTEM_PROMPT}]\n",
        "state = {}\n",
        "\n",
        "# Turn 1: user gives a fact\n",
        "messages.append({\"role\": \"user\", \"content\": \"Remember that my favorite color is blue.\"})\n",
        "messages, state = run_turn(messages, state)\n",
        "print(\"Turn 1 tokens:\", count_tokens(messages))\n",
        "\n",
        "# Turn 2: user asks to recall\n",
        "messages.append({\"role\": \"user\", \"content\": \"What is my favorite color?\"})\n",
        "messages, state = run_turn(messages, state)\n",
        "print(\"Turn 2 tokens:\", count_tokens(messages))\n",
        "\n",
        "# Show last assistant text reply\n",
        "assistant_msgs = [m for m in messages if m.get(\"role\") == \"assistant\" and m.get(\"content\")]\n",
        "if assistant_msgs:\n",
        "    print(\"Last assistant reply:\", assistant_msgs[-1][\"content\"])\n",
        "print(\"State facts:\", state.get(\"facts\", {}))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Expected**: The agent stores \"favorite color\" → \"blue\" and later retrieves it. Token count grows each turn.  \n",
        "**Record**: Total tokens after turn 2; confirm the agent correctly referenced turn 1.\n",
        "\n",
        "---\n",
        "\n",
        "## 5. Exploration Lab: Context Overflow, Summarization, and Structured State\n",
        "\n",
        "### Experiment 1: Context overflow\n",
        "\n",
        "**Variable**: Feed many turns (e.g. 20–50) of conversation so the prompt grows large.  \n",
        "**Hypothesis**: Once the effective context is full or very long, the model may start to contradict earlier statements or \"forget\" facts.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell: Experiment 1 — Many turns (simulated long history)\n",
        "\n",
        "# Simulate a long history by appending many user/assistant pairs\n",
        "messages_long = [{\"role\": \"system\", \"content\": SYSTEM_PROMPT}]\n",
        "state_long = {}\n",
        "for i in range(15):\n",
        "    messages_long.append({\"role\": \"user\", \"content\": f\"Remember that item_{i} is value_{i}.\"})\n",
        "    messages_long, state_long = run_turn(messages_long, state_long)\n",
        "    if i in (0, 5, 10, 14):\n",
        "        print(f\"After turn {i+1}: tokens ≈ {count_tokens(messages_long)}\")\n",
        "\n",
        "# Now ask for an early fact\n",
        "messages_long.append({\"role\": \"user\", \"content\": \"What was the value of item_0?\"})\n",
        "messages_long, state_long = run_turn(messages_long, state_long)\n",
        "assistant = [m for m in messages_long if m.get(\"role\") == \"assistant\" and m.get(\"content\")][-1]\n",
        "print(\"Reply about item_0:\", assistant[\"content\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Observe**: With 15+ turns, token count grows quickly. The model may still answer correctly if it uses the tool to read from `state`; if it relied only on raw history, recency and length can cause errors.  \n",
        "**Record**: Token count at the end; whether the agent correctly recalled `item_0`.\n",
        "\n",
        "---\n",
        "\n",
        "### Experiment 2: Summarization injection\n",
        "\n",
        "**Variable**: Every N turns, replace the middle of the history with an LLM-generated summary.  \n",
        "**Hypothesis**: Summarization keeps the prompt shorter so the model maintains coherence longer; some detail may be lost.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell: Experiment 2 — Summarize history every 5 turns\n",
        "\n",
        "def summarize_messages(messages: list[dict]) -> str:\n",
        "    \"\"\"Ask the model to summarize the conversation so far (user/assistant only).\"\"\"\n",
        "    conv = []\n",
        "    for m in messages:\n",
        "        if m[\"role\"] in (\"user\", \"assistant\") and m.get(\"content\"):\n",
        "            conv.append(f\"{m['role']}: {m['content'][:200]}\")\n",
        "    text = \"\\n\".join(conv[-20:])  # last 20 entries\n",
        "    r = client.chat.completions.create(\n",
        "        model=MODEL,\n",
        "        temperature=0.0,\n",
        "        messages=[{\"role\": \"user\", \"content\": f\"Summarize this conversation in 3–5 sentences, preserving key facts and decisions:\\n\\n{text}\"}],\n",
        "        max_tokens=150,\n",
        "    )\n",
        "    return r.choices[0].message.content.strip()\n",
        "\n",
        "\n",
        "# Build history with summarization every 5 turns\n",
        "messages_sum = [{\"role\": \"system\", \"content\": SYSTEM_PROMPT}]\n",
        "state_sum = {}\n",
        "summary_so_far = None\n",
        "for i in range(12):\n",
        "    messages_sum.append({\"role\": \"user\", \"content\": f\"Remember that project_{i} has status done.\"})\n",
        "    messages_sum, state_sum = run_turn(messages_sum, state_sum)\n",
        "    if (i + 1) % 5 == 0 and i > 0:\n",
        "        summary_so_far = summarize_messages(messages_sum)\n",
        "        # Replace all but system + last few turns with a summary block\n",
        "        keep = [messages_sum[0]] + messages_sum[-(4 * 2):]  # system + last ~4 exchanges\n",
        "        messages_sum = [messages_sum[0], {\"role\": \"user\", \"content\": f\"[Summary of earlier conversation]: {summary_so_far}\"}] + keep[1:]\n",
        "        print(f\"After turn {i+1}: summarized; tokens ≈ {count_tokens(messages_sum)}\")\n",
        "\n",
        "print(\"Sample summary:\", summary_so_far[:200] if summary_so_far else \"N/A\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Observe**: Summaries compress context; the model can still use the tool state for exact values. Compare what the summary preserves vs. drops.  \n",
        "**Record**: Whether the agent still recalls early \"project_k\" values; what the summary contained.\n",
        "\n",
        "---\n",
        "\n",
        "### Experiment 3: Structured scratchpad in system message\n",
        "\n",
        "**Variable**: Maintain a JSON state blob in the system message that the model (or your code) updates each turn.  \n",
        "**Hypothesis**: Explicit state is more reliable for exact values (e.g. ticket IDs) than implicit conversation history.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell: Experiment 3 — Structured scratchpad\n",
        "\n",
        "def build_system_with_scratchpad(scratchpad: dict) -> str:\n",
        "    return (\n",
        "        SYSTEM_PROMPT\n",
        "        + \"\\n\\nCurrent structured state (use this for exact values):\\n\"\n",
        "        + json.dumps(scratchpad, indent=2)\n",
        "    )\n",
        "\n",
        "messages_scratch = []\n",
        "scratchpad = {\"facts\": {}, \"ticket_id_counter\": 0}\n",
        "messages_scratch.append({\"role\": \"system\", \"content\": build_system_with_scratchpad(scratchpad)})\n",
        "messages_scratch.append({\"role\": \"user\", \"content\": \"Create a ticket for 'Server down' and remember its ID.\"})\n",
        "# Simulate: we run tool and update scratchpad\n",
        "scratchpad[\"ticket_id_counter\"] = 1\n",
        "scratchpad[\"facts\"][\"last_ticket_id\"] = \"TKT-001\"\n",
        "scratchpad[\"facts\"][\"last_ticket_title\"] = \"Server down\"\n",
        "messages_scratch[0][\"content\"] = build_system_with_scratchpad(scratchpad)\n",
        "messages_scratch.append({\"role\": \"assistant\", \"content\": \"I've created ticket TKT-001 for 'Server down'.\"})\n",
        "messages_scratch.append({\"role\": \"user\", \"content\": \"What was the last ticket ID?\"})\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=MODEL,\n",
        "    temperature=0.0,\n",
        "    messages=messages_scratch,\n",
        "    max_tokens=50,\n",
        ")\n",
        "print(\"Reply:\", response.choices[0].message.content)\n",
        "print(\"Scratchpad:\", json.dumps(scratchpad, indent=2))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Observe**: The scratchpad holds exact values; the model can answer \"What was the last ticket ID?\" from the system message.  \n",
        "**Record**: Reply correctness. **Implication**: For EOP agents, critical IDs and counts should live in structured state, not only in free-form history.\n",
        "\n",
        "---\n",
        "\n",
        "### Experiment 4: State corruption\n",
        "\n",
        "**Variable**: Intentionally inject conflicting information into the scratchpad (e.g. two different \"last_ticket_id\" values).  \n",
        "**Hypothesis**: Agent behavior becomes unpredictable; demonstrates why state validation and single source of truth matter.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell: Experiment 4 — State corruption\n",
        "\n",
        "scratchpad_bad = {\"facts\": {\"last_ticket_id\": \"TKT-001\", \"current_ticket\": \"TKT-999\"}, \"note\": \"Conflicting IDs\"}\n",
        "sys_corrupt = build_system_with_scratchpad(scratchpad_bad)\n",
        "messages_corrupt = [\n",
        "    {\"role\": \"system\", \"content\": sys_corrupt},\n",
        "    {\"role\": \"user\", \"content\": \"What is the last ticket ID we created?\"},\n",
        "]\n",
        "r = client.chat.completions.create(model=MODEL, temperature=0.0, messages=messages_corrupt, max_tokens=80)\n",
        "print(\"Reply with corrupted state:\", r.choices[0].message.content)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Observe**: The model may pick one value, hedge, or contradict. **Record**: How the model responded. **Implication**: Validate and normalize state; avoid duplicate or conflicting keys for the same concept.\n",
        "\n",
        "---\n",
        "\n",
        "## 6. Maintenance Connection: How This Helps Debug or Scale the EOP Agent\n",
        "\n",
        "### Persistence across sessions\n",
        "\n",
        "- EOP agents often persist state across sessions via **database**, **Redis**, or **serialized JSON**. The in-memory `state` and `messages` in this lab are stand-ins for that.\n",
        "- When debugging \"the agent forgot X\": (1) **Check token count** and context-window limits. (2) **Inspect summarization** — what was dropped? (3) **Prefer structured state** for exact identifiers and counters.\n",
        "\n",
        "### State serialization for replay and debugging\n",
        "\n",
        "- Snapshot the full `messages` list (and state dict) at each decision point. This allows replay and regression tests: \"Given this history, the agent should choose tool Y.\"\n",
        "\n",
        "### Takeaways\n",
        "\n",
        "| Issue | Check | Fix |\n",
        "|-------|--------|-----|\n",
        "| Agent forgot earlier instruction | Token count, history length | Summarize or trim history; move critical facts to structured state |\n",
        "| Inconsistent recall | Recency bias, long context | Put key facts in system scratchpad or tool-maintained state |\n",
        "| State drift | Conflicting keys, no validation | Single source of truth; validate state shape (e.g. Pydantic) before writing |\n",
        "\n",
        "---\n",
        "\n",
        "## 7. Summary and Next Steps\n",
        "\n",
        "### Three takeaways\n",
        "\n",
        "1. **Memory is prompt engineering.** All context is in the prompt; the model has no internal memory. Context window is finite — every message costs tokens.\n",
        "2. **Structured state is more reliable than implicit state.** For ticket IDs, counts, and exact values, maintain a dedicated state object (and optionally expose it in the system message) instead of relying only on conversation history.\n",
        "3. **Summarization and state validation extend coherence.** Compress history when it grows; validate and normalize state to avoid corruption and ambiguous recall.\n",
        "\n",
        "### What's next\n",
        "\n",
        "**Lab 4 — Graphs, Cycles & Recovery**: LangGraph and flow logic. We'll model an agent workflow as a graph with conditional routing, retry cycles, and error recovery.\n",
        "\n",
        "---\n",
        "\n",
        "*End of Lab 3. Proceed to Lab 4 when ready.*\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Lab3_The_Persistent_Agent.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
