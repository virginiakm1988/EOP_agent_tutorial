{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lab 0: Build a Minimal EOP Agent Prototype — Step by Step\n",
        "\n",
        "**Series**: Agentic Engineering Crash Course (EOP focus)  \n",
        "**Goal**: Assemble the simplest agent that understands \"evidence-oriented\" actions and chooses a tool to help researchers annotate or link artifacts.  \n",
        "**Prerequisites**: Python 3.10+, OpenAI API key (or NVIDIA NIM).  \n",
        "**Time**: ~30–40 min.\n",
        "\n",
        "---\n",
        "\n",
        "## What You Will Build\n",
        "\n",
        "By the end of this lab you will have a **single-turn EOP agent** that:\n",
        "\n",
        "1. Reads a user message (e.g. \"Tag this file as input data\" or \"Link Figure 2 to the main claim\").\n",
        "2. Chooses one of two **EOP tools**: `annotate_artifact` or `link_to_claim`.\n",
        "3. Executes the chosen tool and returns a short result.\n",
        "\n",
        "No frameworks (LangChain/LangGraph) — just prompt, LLM call, parse, and execute. This matches the idea from the EOP paper: *AI agents might assist researchers in identifying and annotating evidentiary artifacts during software development*.\n",
        "\n",
        "---\n",
        "\n",
        "## How to Use This Tutorial\n",
        "\n",
        "- Run cells in order from top to bottom.\n",
        "- For terms (prompt, tool call, LLM), see [Glossary](Glossary.md)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Step 1: Setup\n",
        "\n",
        "Install the client and load your API key. Same pattern as Lab 1 (OpenAI or NVIDIA NIM)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -q openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using model: nvidia/llama-3.3-nemotron-super-49b-v1.5\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import re\n",
        "from getpass import getpass\n",
        "from openai import OpenAI\n",
        "\n",
        "use_nim = os.environ.get(\"USE_NIM\", \"\").lower() in (\"1\", \"true\", \"yes\") or \"NIM_API_KEY\" in os.environ\n",
        "use_nim = True\n",
        "if use_nim:\n",
        "    if \"NIM_API_KEY\" not in os.environ:\n",
        "        os.environ[\"NIM_API_KEY\"] = getpass(\"Enter your NVIDIA API key (NIM): \")\n",
        "    client = OpenAI(base_url=\"https://integrate.api.nvidia.com/v1\", api_key=os.environ[\"NIM_API_KEY\"])\n",
        "    MODEL = os.environ.get(\"NIM_MODEL\", \"nvidia/llama-3.3-nemotron-super-49b-v1.5\")\n",
        "else:\n",
        "    if \"OPENAI_API_KEY\" not in os.environ:\n",
        "        os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter your OpenAI API key: \")\n",
        "    client = OpenAI()\n",
        "    MODEL = \"gpt-4o-mini\"\n",
        "\n",
        "print(f\"Using model: {MODEL}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Step 2: Define EOP Tools (Concept Only)\n",
        "\n",
        "We define two tools that match the EOP idea of *identifying and linking evidence*:\n",
        "\n",
        "| Tool | When to use |\n",
        "|------|------------------|\n",
        "| `annotate_artifact` | User wants to tag a file, dataset, or figure/table as part of the evidence chain (e.g. input data, output data, visual claim). |\n",
        "| `link_to_claim`     | User wants to link an artifact or process to a scientific claim. |\n",
        "\n",
        "For this prototype, each tool is a Python function that returns a short message. No real I/O yet.\n",
        "\n",
        "**Check**: You will have a tool registry (`EOP_TOOLS`) and an executor (`run_tool`). The agent's job is to *choose* `tool_name`; we parse it from the LLM output in the next steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "EOP_TOOLS = {\n",
        "    \"annotate_artifact\": \"Tag a file or data artifact as part of the evidence chain (e.g. input_data, output_data, visual_claim). Use when the user mentions a file, dataset, or figure/table they want to annotate.\",\n",
        "    \"link_to_claim\": \"Link an artifact or process to a scientific claim. Use when the user wants to associate evidence with a claim or figure/table with a claim.\",\n",
        "}\n",
        "\n",
        "\n",
        "def execute_annotate_artifact(artifact_name: str = \"\") -> str:\n",
        "    \"\"\"Placeholder: in a full implementation, this would update metadata or a manifest.\"\"\"\n",
        "    return f\"[EOP] Annotated artifact: {artifact_name or '(unspecified)'} — recorded in evidence chain.\"\n",
        "\n",
        "\n",
        "def execute_link_to_claim(artifact_name: str = \"\", claim_text: str = \"\") -> str:\n",
        "    \"\"\"Placeholder: in a full implementation, this would store the artifact–claim link.\"\"\"\n",
        "    return f\"[EOP] Linked '{artifact_name or '(artifact)'}' to claim: '{claim_text or '(claim)'}'.\"\n",
        "\n",
        "\n",
        "def run_tool(tool_name: str, **kwargs) -> str:\n",
        "    \"\"\"Execute the named EOP tool and return a result string.\"\"\"\n",
        "    if tool_name == \"annotate_artifact\":\n",
        "        return execute_annotate_artifact(artifact_name=kwargs.get(\"artifact_name\", \"\"))\n",
        "    if tool_name == \"link_to_claim\":\n",
        "        return execute_link_to_claim(\n",
        "            artifact_name=kwargs.get(\"artifact_name\", \"\"),\n",
        "            claim_text=kwargs.get(\"claim_text\", \"\"),\n",
        "        )\n",
        "    return f\"[EOP] Unknown tool: {tool_name}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Step 3: Build the Prompt and Ask the LLM for a Tool Choice\n",
        "\n",
        "The agent prompt has two parts: (1) system message = \"you have these tools, reply with TOOL: <name>\"; (2) user message = the researcher's request. We send them to the LLM and get back one line like `TOOL: annotate_artifact`.\n",
        "\n",
        "**Observe**: Same idea as Lab 1 — prompt structure and format (e.g. \"TOOL: …\") determine whether the model's answer is easy to parse and correct."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_system_prompt(tools: dict) -> str:\n",
        "    \"\"\"Build the system message that lists EOP tools and the reply format.\"\"\"\n",
        "    tool_names = list(tools.keys())\n",
        "    lines = [\n",
        "        \"You are an EOP (Evidence-Oriented Programming) assistant. You help researchers annotate artifacts and link them to scientific claims.\",\n",
        "        \"\",\n",
        "        \"You have the following tools:\",\n",
        "    ]\n",
        "    for i, (name, desc) in enumerate(tools.items(), 1):\n",
        "        lines.append(f\"  {i}. {name} — {desc}\")\n",
        "    lines.extend([\n",
        "        \"\",\n",
        "        \"Given the user's message, choose exactly one tool to invoke. You must reply with ONLY a single line in this exact format:\",\n",
        "        \"TOOL: <tool_name>\",\n",
        "        \"\",\n",
        "        f\"Valid tool names are: {', '.join(tool_names)}. Do not include reasoning, explanations, or <think> tags — only the line TOOL: <tool_name>.\",\n",
        "    ])\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "\n",
        "def parse_tool_choice(response_text: str):\n",
        "    \"\"\"Extract tool name from a line like 'TOOL: annotate_artifact'. Strips <think> blocks first.\"\"\"\n",
        "    text = response_text.strip()\n",
        "    # Remove <think>...</think>\n",
        "    text = re.sub(r\"<think>.*?</think>\", \"\", text, flags=re.DOTALL)\n",
        "    text = text.strip()\n",
        "    match = re.search(r\"TOOL:\\\\s*(\\\\S+)\", text, re.IGNORECASE)\n",
        "    if match:\n",
        "        return match.group(1)\n",
        "    # Fallback: TOOL: may appear in raw text if <think> was unclosed or truncated\n",
        "    match = re.search(r\"TOOL:\\s*(\\S+)\", response_text, re.IGNORECASE)\n",
        "    return match.group(1) if match else None\n",
        "\n",
        "\n",
        "def ask_agent_for_tool(user_message: str, tools: dict, temperature: float = 0.0):\n",
        "    \"\"\"Send user message to the LLM; return raw response and parsed tool name.\"\"\"\n",
        "    system = build_system_prompt(tools)\n",
        "    response = client.chat.completions.create(\n",
        "        model=MODEL,\n",
        "        temperature=temperature,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system},\n",
        "            {\"role\": \"user\", \"content\": user_message},\n",
        "        ],\n",
        "        max_tokens=150,\n",
        "    )\n",
        "    text = (response.choices[0].message.content or \"\").strip()\n",
        "    tool = parse_tool_choice(text)\n",
        "    return {\"raw\": text, \"tool\": tool}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Step 4: Wire Tool Choice to Execution (One Turn)\n",
        "\n",
        "Combine \"ask LLM\" and \"run tool\" into a single function. For the prototype we do not parse arguments from the LLM; we pass the raw `user_message` as a simple context string so the placeholder tools have something to show."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_eop_agent(user_message: str, tools: dict = None, temperature: float = 0.0):\n",
        "    \"\"\"\n",
        "    One turn: user message -> LLM chooses tool -> we execute tool -> return result.\n",
        "    \"\"\"\n",
        "    if tools is None:\n",
        "        tools = EOP_TOOLS\n",
        "\n",
        "    step1 = ask_agent_for_tool(user_message, tools, temperature=temperature)\n",
        "    chosen = step1[\"tool\"]\n",
        "\n",
        "    if not chosen or chosen not in tools:\n",
        "        return {\n",
        "            \"user_message\": user_message,\n",
        "            \"raw_response\": step1[\"raw\"],\n",
        "            \"chosen_tool\": chosen,\n",
        "            \"tool_result\": None,\n",
        "            \"error\": \"No valid tool parsed or tool not in list.\",\n",
        "        }\n",
        "\n",
        "    # Optional: later you could parse artifact_name / claim_text from user_message or from LLM\n",
        "    tool_result = run_tool(chosen, artifact_name=user_message[:80], claim_text=\"\")\n",
        "\n",
        "    return {\n",
        "        \"user_message\": user_message,\n",
        "        \"raw_response\": step1[\"raw\"],\n",
        "        \"chosen_tool\": chosen,\n",
        "        \"tool_result\": tool_result,\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Step 5: Try Your EOP Agent\n",
        "\n",
        "Run a few example user messages and inspect the chosen tool and the placeholder result."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "User: Tag the file data/measurements.csv as input data for the experiment.\n",
            "LLM said: <think>\n",
            "Okay, the user wants to tag the file data/measurements.csv as input data for the experiment. Let me check the available tools. There's annotate_artifact and link_to_claim. The task is about tagging a file, which is an artifact. The annotate_artifact tool is used for tagging files or data artifacts as part of the evidence chain, like input_data. Since the user specifically mentions tagging the file as input data, I should use annotate_artifact. The link_to_claim is for associating evidence with a claim, which isn't needed here. So the correct tool is annotate_artifact.\n",
            "</think>\n",
            "\n",
            "TOOL: annotate_artifact\n",
            "Chosen tool: annotate_artifact\n",
            "Tool result: [EOP] Annotated artifact: Tag the file data/measurements.csv as input data for the experiment. — recorded in evidence chain.\n",
            "\n",
            "User: Link Figure 2 to the main claim about the correlation.\n",
            "LLM said: <think>\n",
            "Okay, the user wants to link Figure 2 to the main claim about the correlation. Let me see. The available tools are annotate_artifact and link_to_claim. The task is to associate the figure with the claim. Since the figure is already mentioned, maybe it's already annotated. But the user specifically says \"link Figure 2 to the main claim,\" so the action here is linking, not annotating. The link_to_claim tool is for associating evidence (like a figure) with a claim. So the correct tool here is link_to_claim. I should make sure that the figure is the artifact and the claim is about correlation. Yep, that fits. So the answer should be link_to_claim.\n",
            "</think>\n",
            "Chosen tool: None\n",
            "Tool result: None\n",
            "\n",
            "User: I want to annotate the trained model checkpoint as output data.\n",
            "LLM said: <think>\n",
            "Okay, the user wants to annotate the trained model checkpoint as output data. Let me check the available tools. There's annotate_artifact and link_to_claim. The user is specifically talking about tagging a file (the checkpoint) as part of the evidence chain, which fits the annotate_artifact tool. The description for annotate_artifact mentions tagging files or data artifacts like input_data or output_data. Since the user refers to the checkpoint as output data, I should use annotate_artifact. No need for link_to_claim here because they're not connecting it to a claim yet, just annotating the artifact itself.\n",
            "</think>\n",
            "\n",
            "TOOL: annotate_artifact\n",
            "Chosen tool: annotate_artifact\n",
            "Tool result: [EOP] Annotated artifact: I want to annotate the trained model checkpoint as output data. — recorded in evidence chain.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "examples = [\n",
        "    \"Tag the file data/measurements.csv as input data for the experiment.\",\n",
        "    \"Link Figure 2 to the main claim about the correlation.\",\n",
        "    \"I want to annotate the trained model checkpoint as output data.\",\n",
        "]\n",
        "\n",
        "for msg in examples:\n",
        "    out = run_eop_agent(msg, temperature=0.0)\n",
        "    print(\"User:\", out[\"user_message\"])\n",
        "    print(\"LLM said:\", out[\"raw_response\"])\n",
        "    print(\"Chosen tool:\", out[\"chosen_tool\"])\n",
        "    print(\"Tool result:\", out[\"tool_result\"])\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Record**:\n",
        "\n",
        "- For \"Tag the file …\" / \"annotate …\" you should usually see `annotate_artifact`.\n",
        "- For \"Link Figure 2 to the main claim\" you should usually see `link_to_claim`.\n",
        "- If something different happens, note it — that's the kind of behavior Lab 1 teaches you to fix with prompt structure and temperature."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Step 6: Optional — \"No Tool\" and Format Drift\n",
        "\n",
        "Sometimes the user might ask something that doesn't clearly map to a tool (e.g. \"What is EOP?\"). The model might then reply with text that doesn't match `TOOL: <name>`, and `parse_tool_choice` returns `None`. Our agent already handles that by returning `chosen_tool: None` and `error: \"No valid tool parsed...\"`. Try it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chosen tool: None\n",
            "Tool result: None\n",
            "Error: No valid tool parsed or tool not in list.\n"
          ]
        }
      ],
      "source": [
        "out = run_eop_agent(\"What is Evidence-Oriented Programming?\", temperature=0.0)\n",
        "print(\"Chosen tool:\", out[\"chosen_tool\"])\n",
        "print(\"Tool result:\", out[\"tool_result\"])\n",
        "print(\"Error:\", out.get(\"error\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Observe**: When the answer is not in the expected format, the agent \"fails gracefully\" (no crash, but no tool run). Improving this (e.g. a \"no_tool\" or \"answer_directly\" option) is a natural next step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Summary and Next Steps\n",
        "\n",
        "You built a minimal EOP agent that:\n",
        "\n",
        "1. **Setup**: Connects to an LLM (OpenAI or NIM).\n",
        "2. **Tools**: Defines two EOP-themed tools and executes them via `run_tool`.\n",
        "3. **Prompt**: Builds a system message that lists tools and asks for `TOOL: <name>`.\n",
        "4. **Parse**: Extracts the tool name from the model output.\n",
        "5. **Run**: Calls `run_eop_agent(user_message)` → LLM chooses tool → execute → return result.\n",
        "\n",
        "**Takeaways**:\n",
        "\n",
        "- The agent is a loop: *user message → prompt (system + user) → LLM → parse tool → execute tool*. This is the same anatomy you see in Lab 1; here we added execution.\n",
        "- EOP tools are just functions; the \"evidence chain\" is only simulated (placeholder messages). A real implementation would write to a manifest or database.\n",
        "- Prompt structure and reply format matter: if you change the wording or the \"TOOL: …\" convention, parsing can break (format drift).\n",
        "\n",
        "**Next**:\n",
        "\n",
        "- **Lab 1** — Understand why the model sometimes picks the wrong tool (order, temperature, vague prompts) and how to debug.\n",
        "- **Lab 2** — Define tools with a proper schema (e.g. Pydantic) so the model can return *arguments* (e.g. `artifact_name`, `claim_text`) and you can pass them into `run_tool` instead of the raw message.\n",
        "\n",
        "---\n",
        "\n",
        "*For the full series, see Lab 1–6 in the Agentic Engineering Crash Course and the EOP/ECF materials.*"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
