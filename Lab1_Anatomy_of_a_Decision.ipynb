{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lab 1: The Anatomy of a Decision\n",
        "## Prompt Structure and Tool Selection Accuracy\n",
        "\n",
        "**Series**: Agentic Engineering Crash Course \n",
        "**Module**: 1 — Tokenization & Logit Control (Understanding Stochasticity) \n",
        "**Prerequisites**: Python 3.10+, OpenAI API key \n",
        "\n",
        "---\n",
        "\n",
        "**Suggested time**: 45–60 min.  \n",
        "**Experiments**: Baseline (required). Exploration: Experiments 1–3 required; Experiment 4 optional.\n",
        "\n",
        "For definitions of key terms (e.g. logits, token, context window, tool call), see [Glossary](Glossary.md).\n",
        "\n",
        "### Learning Objectives\n",
        "\n",
        "By the end of this lab you will be able to:\n",
        "\n",
        "1. **Explain** why prompt structure changes which tool a language model selects.\n",
        "2. **Observe** that tool choice is a discrete decision governed by a conditional probability distribution over tokens.\n",
        "3. **Run controlled experiments** that isolate the effect of definition order, prompt clarity, temperature, and format drift on tool selection.\n",
        "4. **Connect** these observations to real debugging workflows for EOP (Enterprise Operations) Agents."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 前置概念（Before You Start）\n",
        "\n",
        "If you are new to LLMs and agents, these minimal concepts will help you follow the lab:\n",
        "\n",
        "- **Prompt**: The text (and structure) you send to the model. It usually includes a system message (instructions, tool list) and a user message (the current request). The model's reply is *conditioned* on this entire prompt.\n",
        "- **LLM (Large Language Model)**: A model that takes text as input and produces text (or structured output) as output. It has no memory between calls; each response depends only on what you send in that request.\n",
        "- **API call**: You use a service (e.g. OpenAI) by sending an HTTP request with your prompt and API key. The service runs the model and returns the completion. This lab uses the OpenAI Python client to make these calls.\n",
        "- **Token**: The basic unit of text the model processes (roughly words or subwords). Models have a fixed **context window** (max tokens per request); your prompt and response must fit within it.\n",
        "\n",
        "You do **not** need prior knowledge of neural networks, attention, or softmax to complete this lab."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 1. Theoretical Why: How Prompts Shape Tool Selection\n",
        "\n",
        "### 1.1 Tool Selection as Conditional Decoding\n",
        "\n",
        "When an LLM is presented with a set of tools and a user query, it does not \"choose\" a tool the way a programmer writes an `if/else` branch. Instead, it produces a **probability distribution over the next tokens**, conditioned on the entire prompt context:\n",
        "\n",
        "$$P(\\text{tool}_i \\mid \\text{system message}, \\text{tool definitions}, \\text{user query})$$\n",
        "\n",
        "The \"tool selection\" is the result of decoding (sampling or argmax) from this distribution. Change the conditioning context — reorder the tool list, rephrase the user query, alter the system message — and you change the distribution.\n",
        "\n",
        "### 1.2 Key Mechanisms\n",
        "\n",
        "**Context window and attention.** The model's self-attention layers assign different weights to different parts of the prompt. Tool definitions placed at the end of the system message may receive stronger attention than those buried in the middle (recency bias). Definitions placed first may benefit from primacy effects depending on the model architecture and fine-tuning.\n",
        "\n",
        "**Format alignment.** Models are fine-tuned on specific prompt formats (e.g., `{\"role\": \"system\", \"content\": \"...\"}` for chat models). If your prompt deviates from the expected format — for example, by omitting the explicit instruction to respond with a tool call — the model may fall back to free-text generation instead of structured tool invocation. We call this **format drift**.\n",
        "\n",
        "**Stochasticity.** At temperature > 0, the model *samples* from the distribution rather than taking the argmax. The same prompt can yield different tool selections across runs. Prompt design reduces variance by making the correct tool's probability mass dominant.\n",
        "\n",
        "### 1.3 Maintenance Connection (Preview)\n",
        "\n",
        "When an EOP Agent selects the wrong tool in production, the diagnostic checklist starts here:\n",
        "\n",
        "1. **Prompt structure** — Is the system message well-formed? Are tool definitions ordered and described clearly?\n",
        "2. **Sampling parameters** — Is temperature set appropriately for this decision point?\n",
        "3. **Format contract** — Does the prompt enforce the expected output format?\n",
        "\n",
        "This lab gives you the empirical foundation to reason about each of these."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 2. Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# --- Cell: Install dependencies ---\n",
        "!pip install -q openai"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# --- Cell: Imports and API key ---\n",
        "import os\n",
        "import json\n",
        "import re\n",
        "from collections import Counter\n",
        "from getpass import getpass\n",
        "\n",
        "from openai import OpenAI\n",
        "\n",
        "# Prompt for API key (never hard-code secrets)\n",
        "if \"OPENAI_API_KEY\" not in os.environ:\n",
        "    os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter your OpenAI API key: \")\n",
        "\n",
        "client = OpenAI()\n",
        "\n",
        "MODEL = \"gpt-4o-mini\"  # Affordable, sufficient for this lab\n",
        "print(f\"Using model: {MODEL}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 3. Baseline Code: Minimal Tool Selection\n",
        "\n",
        "We define two simple tools as plain-text descriptions in the system prompt, ask the model to reply with a structured `TOOL: <name>` line, and parse the result. No framework — just raw prompt engineering."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# --- Cell: Tool definitions and prompt template ---\n",
        "\n",
        "TOOLS = {\n",
        "    \"get_weather\": \"Retrieve the current weather for a given city. Use when the user asks about weather, temperature, or forecast.\",\n",
        "    \"search_docs\": \"Search internal documentation by keyword. Use when the user asks about policies, procedures, or technical references.\",\n",
        "}\n",
        "\n",
        "\n",
        "def build_system_prompt(tools: dict[str, str]) -> str:\n",
        "    \"\"\"Build a system message listing the available tools.\"\"\"\n",
        "    tool_block = \"\\n\".join(\n",
        "        f\"  {i+1}. {name} — {desc}\" for i, (name, desc) in enumerate(tools.items())\n",
        "    )\n",
        "    return (\n",
        "        \"You are a tool-routing assistant. You have the following tools:\\n\"\n",
        "        f\"{tool_block}\\n\\n\"\n",
        "        \"Given the user's message, decide which single tool to invoke.\\n\"\n",
        "        \"Reply with exactly one line in the format:\\n\"\n",
        "        \"TOOL: <tool_name>\\n\"\n",
        "        \"Do not include any other text.\"\n",
        "    )\n",
        "\n",
        "\n",
        "def parse_tool_choice(response_text: str) -> str | None:\n",
        "    \"\"\"Extract the tool name from a 'TOOL: <name>' response.\"\"\"\n",
        "    match = re.search(r\"TOOL:\\s*(\\S+)\", response_text, re.IGNORECASE)\n",
        "    return match.group(1) if match else None\n",
        "\n",
        "\n",
        "# Preview the system prompt\n",
        "system_prompt = build_system_prompt(TOOLS)\n",
        "print(system_prompt)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# --- Cell: Single tool-selection call ---\n",
        "\n",
        "def select_tool(\n",
        "    user_message: str,\n",
        "    tools: dict[str, str],\n",
        "    temperature: float = 0.0,\n",
        "    model: str = MODEL,\n",
        ") -> dict:\n",
        "    \"\"\"\n",
        "    Send a prompt to the model and return the raw response,\n",
        "    the parsed tool choice, and the prompt used.\n",
        "    \"\"\"\n",
        "    system = build_system_prompt(tools)\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        temperature=temperature,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system},\n",
        "            {\"role\": \"user\", \"content\": user_message},\n",
        "        ],\n",
        "        max_tokens=30,\n",
        "    )\n",
        "    text = response.choices[0].message.content.strip()\n",
        "    return {\n",
        "        \"user_message\": user_message,\n",
        "        \"raw_response\": text,\n",
        "        \"parsed_tool\": parse_tool_choice(text),\n",
        "        \"temperature\": temperature,\n",
        "    }\n",
        "\n",
        "\n",
        "# --- Baseline run ---\n",
        "result = select_tool(\"What's the weather in New York?\", TOOLS)\n",
        "print(f\"User message : {result['user_message']}\")\n",
        "print(f\"Raw response : {result['raw_response']}\")\n",
        "print(f\"Parsed tool  : {result['parsed_tool']}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Expected output**: `TOOL: get_weather`. The prompt is clear, the user query unambiguously matches one tool, and temperature is 0 (deterministic argmax).\n",
        "\n",
        "> **Record**: Note the parsed tool. This is our **control** result."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 4. Exploration Lab: When Decisions Break\n",
        "\n",
        "Each experiment below isolates a single variable. Run each one, record the results, and compare to the baseline.\n",
        "\n",
        "### Helper: Batch Runner\n",
        "\n",
        "We'll reuse this helper to run the same query N times and tally tool selections."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# --- Cell: Batch helper ---\n",
        "\n",
        "def run_batch(\n",
        "    user_message: str,\n",
        "    tools: dict[str, str],\n",
        "    n: int = 10,\n",
        "    temperature: float = 0.0,\n",
        "    label: str = \"\",\n",
        ") -> dict:\n",
        "    \"\"\"\n",
        "    Run `select_tool` n times and return a frequency table of parsed tool choices.\n",
        "    \"\"\"\n",
        "    results = [\n",
        "        select_tool(user_message, tools, temperature=temperature)\n",
        "        for _ in range(n)\n",
        "    ]\n",
        "    choices = [r[\"parsed_tool\"] for r in results]\n",
        "    freq = Counter(choices)\n",
        "    print(f\"\\n--- {label or user_message} (n={n}, temp={temperature}) ---\")\n",
        "    for tool, count in freq.most_common():\n",
        "        pct = count / n * 100\n",
        "        print(f\"  {tool or 'PARSE_FAIL'}: {count}/{n} ({pct:.0f}%)\")\n",
        "    # show a sample raw response for inspection\n",
        "    print(f\"  Sample raw: {results[0]['raw_response']}\")\n",
        "    return {\"label\": label, \"freq\": dict(freq), \"sample_raw\": results[0][\"raw_response\"]}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Experiment 1: Definition Order Sensitivity\n",
        "\n",
        "**Hypothesis**: Swapping the order of tool definitions in the system prompt may bias the model toward one tool over another, even when the user query is unambiguous.\n",
        "\n",
        "**Variable**: Order of tool definitions. \n",
        "**Control**: Everything else held constant (same user message, temperature = 0)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# --- Cell: Experiment 1 — Order sensitivity ---\n",
        "\n",
        "# Original order: get_weather first\n",
        "tools_order_A = {\n",
        "    \"get_weather\": TOOLS[\"get_weather\"],\n",
        "    \"search_docs\": TOOLS[\"search_docs\"],\n",
        "}\n",
        "\n",
        "# Reversed order: search_docs first\n",
        "tools_order_B = {\n",
        "    \"search_docs\": TOOLS[\"search_docs\"],\n",
        "    \"get_weather\": TOOLS[\"get_weather\"],\n",
        "}\n",
        "\n",
        "query = \"What's the weather in New York?\"\n",
        "\n",
        "res_A = run_batch(query, tools_order_A, n=5, temperature=0.0, label=\"Order A (weather first)\")\n",
        "res_B = run_batch(query, tools_order_B, n=5, temperature=0.0, label=\"Order B (docs first)\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **Observe**: With temperature=0 and an unambiguous query, order likely has no effect — both return `get_weather` 100% of the time. This is the **easy case**. Order effects emerge with ambiguous queries (Experiment 2) or higher temperature (Experiment 3).\n",
        ">\n",
        "> **Record**: Were the results identical across orders? Document yes/no."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Experiment 2: Vague vs. Clear User Prompt\n",
        "\n",
        "**Hypothesis**: A vague user query increases ambiguity in the conditional distribution, leading to less consistent (and potentially wrong) tool selections.\n",
        "\n",
        "**Variable**: User message clarity. \n",
        "**Control**: Same tools, same temperature (0.7 to reveal variance)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# --- Cell: Experiment 2 — Vague vs. clear prompt ---\n",
        "\n",
        "CLEAR_QUERY = \"What's the weather in New York?\"\n",
        "VAGUE_QUERY = \"Help me\"\n",
        "AMBIGUOUS_QUERY = \"I need information\"  # Could be weather OR docs\n",
        "\n",
        "res_clear = run_batch(CLEAR_QUERY, TOOLS, n=10, temperature=0.7, label=\"Clear query\")\n",
        "res_vague = run_batch(VAGUE_QUERY, TOOLS, n=10, temperature=0.7, label=\"Vague query\")\n",
        "res_ambig = run_batch(AMBIGUOUS_QUERY, TOOLS, n=10, temperature=0.7, label=\"Ambiguous query\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **Observe**: \n",
        "> - The **clear query** should still yield `get_weather` in the vast majority of runs, even at temperature 0.7.\n",
        "> - The **vague query** (\"Help me\") has no signal favoring either tool. Expect a mixed distribution — sometimes `get_weather`, sometimes `search_docs`, possibly parse failures.\n",
        "> - The **ambiguous query** (\"I need information\") may lean toward `search_docs` but with nontrivial variance.\n",
        ">\n",
        "> **Record**: Tally each distribution. This demonstrates that prompt clarity is a **variance reducer**.\n",
        ">\n",
        "> **Implication for EOP Agents**: If the user-facing interface allows freeform input, the agent system prompt must compensate with disambiguation instructions or a fallback/clarification tool."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Experiment 3: Temperature as a Variance Dial\n",
        "\n",
        "**Hypothesis**: Increasing temperature spreads probability mass across tool choices, increasing variance. At temperature = 0, the model is deterministic (argmax).\n",
        "\n",
        "**Variable**: Temperature \\(\\in \\{0.0, 0.3, 0.7, 1.2\\}\\). \n",
        "**Control**: Same tools, same user message (deliberately slightly ambiguous)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# --- Cell: Experiment 3 — Temperature sweep ---\n",
        "\n",
        "PROBE_QUERY = \"I need some information about conditions outside.\"\n",
        "# Deliberately ambiguous: \"conditions outside\" could map to weather or docs.\n",
        "\n",
        "temps = [0.0, 0.3, 0.7, 1.2]\n",
        "temp_results = {}\n",
        "\n",
        "for t in temps:\n",
        "    temp_results[t] = run_batch(\n",
        "        PROBE_QUERY, TOOLS, n=10, temperature=t, label=f\"temp={t}\"\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **Observe**:\n",
        "> - At `temperature=0.0`, the result should be identical across all 10 runs (deterministic).\n",
        "> - As temperature increases, you should see the minority tool appearing more frequently.\n",
        "> - At very high temperature (1.2), parse failures may appear — the model's output becomes less structured.\n",
        ">\n",
        "> **Record**: For each temperature, note the majority tool and its percentage. Plot mentally (or literally) the relationship: *temperature vs. selection entropy*.\n",
        ">\n",
        "> **Implication for EOP Agents**: Critical routing decisions (e.g., \"escalate to human\" vs. \"auto-resolve\") should use low temperature. Creative generation steps can tolerate higher temperature."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Experiment 4: Format Drift\n",
        "\n",
        "**Hypothesis**: Removing the explicit format instruction (`TOOL: <name>`) from the system prompt causes the model to revert to free-text, breaking the parser.\n",
        "\n",
        "**Variable**: Presence of the format instruction. \n",
        "**Control**: Same tools, same user message, temperature = 0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# --- Cell: Experiment 4 — Format drift ---\n",
        "\n",
        "def build_system_prompt_no_format(tools: dict[str, str]) -> str:\n",
        "    \"\"\"System prompt WITHOUT the explicit format instruction.\"\"\"\n",
        "    tool_block = \"\\n\".join(\n",
        "        f\"  {i+1}. {name} — {desc}\" for i, (name, desc) in enumerate(tools.items())\n",
        "    )\n",
        "    return (\n",
        "        \"You are a helpful assistant. You have access to the following tools:\\n\"\n",
        "        f\"{tool_block}\\n\\n\"\n",
        "        \"Please help the user with their request.\"\n",
        "        # NOTE: No format instruction. No \"Reply with TOOL: <name>\".\n",
        "    )\n",
        "\n",
        "\n",
        "def select_tool_no_format(\n",
        "    user_message: str,\n",
        "    tools: dict[str, str],\n",
        "    temperature: float = 0.0,\n",
        "    model: str = MODEL,\n",
        ") -> dict:\n",
        "    \"\"\"Like select_tool but uses the broken prompt.\"\"\"\n",
        "    system = build_system_prompt_no_format(tools)\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        temperature=temperature,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system},\n",
        "            {\"role\": \"user\", \"content\": user_message},\n",
        "        ],\n",
        "        max_tokens=100,  # more tokens since free-text may be verbose\n",
        "    )\n",
        "    text = response.choices[0].message.content.strip()\n",
        "    return {\n",
        "        \"user_message\": user_message,\n",
        "        \"raw_response\": text,\n",
        "        \"parsed_tool\": parse_tool_choice(text),  # will likely return None\n",
        "        \"temperature\": temperature,\n",
        "    }\n",
        "\n",
        "\n",
        "# Run structured vs. unstructured\n",
        "query = \"What's the weather in New York?\"\n",
        "\n",
        "print(\"=== WITH format instruction ===\")\n",
        "for _ in range(3):\n",
        "    r = select_tool(query, TOOLS, temperature=0.0)\n",
        "    print(f\"  Parsed: {r['parsed_tool']!r:20s} Raw: {r['raw_response']}\")\n",
        "\n",
        "print(\"\\n=== WITHOUT format instruction ===\")\n",
        "for _ in range(3):\n",
        "    r = select_tool_no_format(query, TOOLS, temperature=0.0)\n",
        "    print(f\"  Parsed: {r['parsed_tool']!r:20s} Raw: {r['raw_response'][:120]}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **Observe**:\n",
        "> - With the format instruction, the parser succeeds every time.\n",
        "> - Without it, the model likely responds with a natural-language sentence (e.g., \"I can help you check the weather...\"). The parser returns `None` — a **silent failure**.\n",
        ">\n",
        "> **This is format drift**: the interface contract between the prompt and the parser is broken. The model is not wrong (it understood the user's intent), but the *system* fails because the output is not machine-parseable.\n",
        ">\n",
        "> **Implication for EOP Agents**: Every tool-routing prompt needs an explicit, tested format contract. When debugging \"the agent didn't call any tool,\" check for format drift *before* blaming the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Experiment 5 (Bonus): Tool Count Scaling\n",
        "\n",
        "What happens when we add more tools? Does selection accuracy degrade?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# --- Cell: Experiment 5 — Scaling tool count ---\n",
        "\n",
        "MANY_TOOLS = {\n",
        "    \"get_weather\": \"Retrieve the current weather for a given city.\",\n",
        "    \"search_docs\": \"Search internal documentation by keyword.\",\n",
        "    \"send_email\": \"Send an email to a specified recipient.\",\n",
        "    \"create_ticket\": \"Create a support ticket in the ticketing system.\",\n",
        "    \"run_query\": \"Execute a SQL query against the analytics database.\",\n",
        "    \"schedule_meeting\": \"Schedule a calendar meeting with attendees.\",\n",
        "    \"translate_text\": \"Translate text from one language to another.\",\n",
        "    \"summarize_page\": \"Summarize a given Confluence page.\",\n",
        "}\n",
        "\n",
        "query_weather = \"What's the weather in Tokyo?\"\n",
        "query_ticket  = \"I need to report a bug in the login flow.\"\n",
        "query_vague   = \"Can you help me with something?\"\n",
        "\n",
        "print(\"--- 8 tools, clear weather query ---\")\n",
        "run_batch(query_weather, MANY_TOOLS, n=5, temperature=0.0, label=\"8 tools / weather\")\n",
        "\n",
        "print(\"\\n--- 8 tools, clear ticket query ---\")\n",
        "run_batch(query_ticket, MANY_TOOLS, n=5, temperature=0.0, label=\"8 tools / ticket\")\n",
        "\n",
        "print(\"\\n--- 8 tools, vague query, temp=0.7 ---\")\n",
        "run_batch(query_vague, MANY_TOOLS, n=10, temperature=0.7, label=\"8 tools / vague\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **Observe**:\n",
        "> - Clear queries should still route correctly even with 8 tools.\n",
        "> - Vague queries with many tools at higher temperature will show increased dispersion — the model \"spreads its bets\" across more candidates.\n",
        ">\n",
        "> **Record**: Compare the vague-query distribution between 2 tools (Experiment 2) and 8 tools (here). More tools + vague query = higher entropy.\n",
        ">\n",
        "> **Scaling insight**: As an EOP Agent grows from 5 to 50 tools, maintaining selection accuracy requires: (a) clear, non-overlapping tool descriptions, (b) hierarchical routing (categories → tools), and (c) logging tool-selection distributions for regression testing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 5. Maintenance Connection: Debugging EOP Agent Tool Selection\n",
        "\n",
        "You now have empirical evidence for four failure modes in tool selection. Here is the diagnostic protocol for an EOP Agent that selects the wrong tool:\n",
        "\n",
        "### Diagnostic Checklist\n",
        "\n",
        "| Step | Check | What to look for | Fix |\n",
        "|------|-------|------------------|-----|\n",
        "| 1 | **Prompt structure** | System message well-formed? Tool definitions present and ordered? | Reformat system prompt; ensure tool list is explicit. |\n",
        "| 2 | **Tool descriptions** | Are two tools' descriptions overlapping or ambiguous? | Rewrite descriptions to be mutually exclusive. |\n",
        "| 3 | **User input clarity** | Is the user query vague or ambiguous? | Add a disambiguation step or a `clarify` tool. |\n",
        "| 4 | **Temperature** | Is the routing step using temperature > 0? | Set `temperature=0` for deterministic routing. |\n",
        "| 5 | **Format contract** | Does the prompt enforce a parseable output format? Does the parser handle edge cases? | Add explicit format instructions; add fallback parsing. |\n",
        "| 6 | **Tool count** | Has the tool set grown beyond ~10 without reorganization? | Introduce hierarchical routing (category → tool). |\n",
        "\n",
        "### Logging Recommendation\n",
        "\n",
        "For any production agent, log the following on every tool-selection decision:\n",
        "\n",
        "```python\n",
        "log_entry = {\n",
        "    \"timestamp\": ...,\n",
        "    \"prompt_hash\": hash(system_prompt),       # detect prompt template changes\n",
        "    \"user_message\": user_message,\n",
        "    \"selected_tool\": parsed_tool,\n",
        "    \"raw_response\": raw_response,              # for post-hoc analysis\n",
        "    \"temperature\": temperature,\n",
        "    \"model\": model_name,\n",
        "}\n",
        "```\n",
        "\n",
        "This enables A/B testing of prompt variants and regression detection when the model is updated.\n",
        "\n",
        "### Connection to Module 1 (Tokenization & Logit Control)\n",
        "\n",
        "In this lab we manipulated tool selection at the **prompt level** — changing what the model sees. In the next module, we go deeper: examining how the model **tokenizes** tool names and how we can apply **logit bias** or **constrained decoding** to force valid tool selections at the token level. The prompt shapes the distribution; logit control narrows it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "### Connection: EOP Agent Capabilities and What You Learned Here\n",
        "\n",
        "An **EOP (Evidence-Oriented Programming) Agent** is an AI assistant that helps researchers adopt evidence-oriented practices: it **advocates** the value of EOP/ECF so users want to use it, **restructures** research code into evidence-chain–friendly layouts (e.g. work / output / claim / source), and **assists** with day-to-day coding. The skills you practiced in this lab map directly onto making such an agent reliable.\n",
        "\n",
        "| EOP Agent capability | How it ties to this lab |\n",
        "|-----------------------|--------------------------|\n",
        "| **Advocacy** (explaining EOP so users want to use it) | The agent’s “advocacy” replies are just another kind of **generation conditioned on the prompt**. Getting consistent, on-message advocacy (e.g. when to explain evidence chains vs when to suggest restructure) depends on **prompt structure**: system message, few-shot examples, and clear boundaries between “explain concept” vs “suggest action.” Same principles as tool routing: **order and clarity** in the prompt shape the distribution over outputs. |\n",
        "| **Restructure code** (reorganizing repos into EOP/ECF layout) | Restructuring requires the agent to **choose the right tools** (e.g. read file, list directory, edit file, suggest layout). Wrong tool ⇒ wrong restructure. Everything you learned applies: **tool definition order** and **non-overlapping descriptions** (e.g. “suggest_directory_layout” vs “edit_file”) reduce misrouting; **format contract** ensures the agent’s suggested changes are parseable and executable. |\n",
        "| **Assist coding** (helping users write or refactor code) | Again the agent must **route** among tools (read, search, edit, run tests). Vague user requests (“make this cleaner”) lead to **inconsistent tool choice** unless the system prompt includes disambiguation or a clarify step. **Temperature** for these routing steps should be set with the same care as in our experiments — low for critical routing, higher only where creativity is desired. |\n",
        "\n",
        "**Bottom line:** For an EOP agent, “selecting the right tool” is the same conditional decoding step we studied. When the agent misbehaves (wrong tool, off-topic advocacy, or broken restructure), the **diagnostic checklist from Section 5** applies: inspect prompt structure, tool order and descriptions, user message clarity, temperature, and format contract. This lab is the foundation for debugging and improving that behavior."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 6. Summary\n",
        "\n",
        "### Three Takeaways\n",
        "\n",
        "1. **Prompt structure conditions the distribution over tool choices.** The order of tool definitions, the wording of the system message, and the presence of format instructions all change which tool the model selects.\n",
        "\n",
        "2. **Clarity and format consistency reduce variance.** A clear user query + an explicit format contract make tool selection near-deterministic, even at moderate temperature.\n",
        "\n",
        "3. **Systematic experiments are the right diagnostic method.** When an agent misbehaves, isolate variables (order, clarity, temperature, format) and measure selection distributions — do not guess.\n",
        "\n",
        "### What's Next\n",
        "\n",
        "**Lab 2: The Contract of a Tool** — We move from string-based tool definitions to **Pydantic schemas** and the **OpenAI function-calling API**, making tool selection and argument parsing formally structured and validated.\n",
        "\n",
        "---\n",
        "\n",
        "*End of Lab 1. Proceed to Lab 2 when ready.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}